{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b8cf4e-d8e0-4af4-962c-7d0c812b3441",
   "metadata": {},
   "source": [
    "Data Wrangling Template\n",
    "\n",
    "Prepared by;\n",
    "\n",
    "Name: Abdur Rahim Khan \n",
    "\n",
    "ERP: 19760"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42543659-64d3-4d42-9c0e-ad4d437d0926",
   "metadata": {},
   "source": [
    "1. Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097d52da-3ed1-4907-9ce0-d42fcbb39e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb9fe7-c982-49c5-bcda-d10688d4b103",
   "metadata": {},
   "source": [
    "2. Reading the Dataset and getting a look and feel of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041eebe1-84cd-4f25-9381-c8fad15f937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a .csv file to pandas dataframe\n",
    "df = pd.read_csv('file.csv')\n",
    "# Read a .json file to pandas dataframe\n",
    "df = pd.read_json('file.json')\n",
    "# Read an excel file to pandas dataframe\n",
    "df = pd.read_excel('file.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50c52a-c505-4cc2-84a8-8a23fd1d2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a .csv with delimiters such as with spaces, tabs or special characters\n",
    "df = pd.read_csv('file.csv',  sep='[:,|_]', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7f773-3d33-4b3c-a5d6-40d99eab12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing unnecessary characters with NaN\n",
    "data_entry_err=['??','na','X','999999']\n",
    "new_df=df.replace(data_entry_err ,np.NaN)\n",
    "display(new_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267bf03-46f8-4ce8-aabb-563bd594f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the idea of the no. of rows and columns in the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b440a62-40f7-40b0-aa82-6cd109ef3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard information of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35b8e2-413e-4f97-b42e-0d0dc16c146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350dbe2-2e46-4c96-b1f2-067bbd9b99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a look and feel of the data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd13b1-d992-45d9-8020-a9c02585499e",
   "metadata": {},
   "source": [
    "3. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f5405-5d8b-4dce-9599-1104fee2182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the no. of missing values for each column and display a bar plot\n",
    "df.isnull().sum()\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367eb9d-0ff8-4753-936a-3f6516923990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of all the rows with missing values\n",
    "rows_with_nan = [index for index, row in df.iterrows() if row.isnull().any()]\n",
    "print(rows_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bc27b-cf32-4920-8ca8-8f0de87aa529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the number of rows with missing values for each column\n",
    "for i in range(dataframe.shape[1]):\n",
    "# count number of rows with missing values\n",
    "n_miss = dataframe[[i]].isnull().sum()\n",
    "perc = n_miss / dataframe.shape[0] * 100\n",
    "print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83201ed-d0f5-4497-986e-2a51c0e3cad7",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347df789-a285-457f-9f1f-2fec8993da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only, columns with missing values, the total number along with the %age\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23faa66d-55c2-49ea-8204-c0483247061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing value stats of columns with missing values\n",
    "Missing_df = missing_values_table(df)\n",
    "display(Missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4854e3b-805e-4cca-9211-65b93fc1882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of missing values\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a27570-72da-46de-85f7-078b55da5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of missingness\n",
    "msno.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f79542-c1e8-470d-9ca3-702426954e51",
   "metadata": {},
   "source": [
    "Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ada43-2654-45f8-9398-c8ec90b2f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns based on a missing value threshold\n",
    "def remove_column_missing_val(df, thr):\n",
    "    pct_null = df.isnull().sum() / len(df)\n",
    "    missing_features = pct_null[pct_null > thr].index\n",
    "    new_df = df.drop(missing_features, axis=1, inplace=True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c15fe-02cb-4955-ac58-621893e075ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = remove_column_missing_val(df, thr)\n",
    "print(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c82088-86b3-4958-8b5f-c7c70c1b335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows based on a missing value threshold\n",
    "def remove_row_missing_val(df, thr):\n",
    "    perc = thr\n",
    "    min_count =  int(((100-perc)/100)*df.shape[1] + 1)\n",
    "    mod_df = df.dropna( axis=0, thresh=min_count)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2a281-9945-457c-9fbd-ad2582195668",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = remove_row_missing_val(df, thr)\n",
    "print(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce7f24-41a5-41f9-8313-be60251a4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing data type of columns. (Based on inuition)\n",
    "new_df = df.astype({'Column1': 'dtype2', 'Column2': 'dtype2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a52a2b-973d-4362-abd7-7a00350e7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Year, Quarter, Month, Day from the Date column\n",
    "df['Dates'] = pd.to_datetime(df['Date']).dt.date\n",
    "df['Time'] = pd.to_datetime(df['Date']).dt.time\n",
    "\n",
    "df['Year'] = df['Dates'].dt.year\n",
    "df['Quarter'] = df['Dates'].dt.quarter\n",
    "df['Month'] = df['Dates'].dt.month\n",
    "df['Day'] = df['Dates'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516fa13-af33-47d2-93ba-5c11fda683d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical and categorical columns separately\n",
    "def get_num_cols(df):\n",
    "    types_map = df.dtypes.to_dict()\n",
    "    num_columns = []\n",
    "    for k,v in types_map.items():\n",
    "        if np.issubdtype(np.int64, v) or np.issubdtype(np.float64, v):\n",
    "            num_columns.append(k)\n",
    "\n",
    "    #print(num_columns)\n",
    "    return num_columns\n",
    "\n",
    "def get_cat_cols(df):\n",
    "    types_map = df.dtypes.to_dict()\n",
    "    cat_columns = []\n",
    "    for k,v in types_map.items():\n",
    "        if not( np.issubdtype(np.int64, v) or np.issubdtype(np.float64, v)):\n",
    "            cat_columns.append(k)\n",
    "\n",
    "    #print(cat_columns)\n",
    "    return cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c4f30-e7e3-4922-9650-0ebe6aa8a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = get_num_cols(df)\n",
    "print(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a7d7c-214b-4942-91e0-e8d78f3d6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = get_cat_cols(df)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea97d5-37b5-4974-964e-2fb8b1cdd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with numerical columns \n",
    "df_num = df.select_dtypes(include = ['int64','float64'])\n",
    "\n",
    "# Create a dataframe with categorical columns \n",
    "df_cat = df.select_dtypes(include =['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c7fd3-24b6-43d6-9bff-69d6805c707f",
   "metadata": {},
   "source": [
    "Outliers & Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecc571-f785-418a-82c0-77a0b14921e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To manually check if each column has outliers or not\n",
    "\n",
    "def check_outliers(df, col):\n",
    "    lower_lim, upper_lim = deter_outlier_thresh_iqr(df, col)\n",
    "    if df[(df[col] > upper_lim) | (df[col] < lower_lim)].any(axis=None):\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe091e-6a0d-406d-9b23-c1b2da4bb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers using IQR\n",
    "def replace_with_thresh(df,cols, th1=0.05, th3=0.95, replace=False):\n",
    "    from tabulate import tabulate\n",
    "    data = []\n",
    "    for col in cols:\n",
    "        if col != 'Outcome':\n",
    "            outliers_ = check_outliers_iqr(df,col)\n",
    "            count = None\n",
    "            lower_lim, upper_lim = determine_outlier_thresh(df, col, th1, th3)\n",
    "            if outliers_:\n",
    "                count = df[(df[col] > upper_lim) | (df[col] < lower_lim)][col].count()\n",
    "                if replace: \n",
    "                    if lower_lim < 0:\n",
    "                        # We don't want to replace with negative values, right!\n",
    "                        df.loc[(df[col] > upper_lim), col] = upper_lim\n",
    "                    else:\n",
    "                        df.loc[(df[col] < lower_lim), col] = lower_lim\n",
    "                        df.loc[(df[col] > upper_lim), col] = upper_lim\n",
    "            outliers_status = check_outliers(df, col)\n",
    "            data.append([outliers_, outliers_status, count, col, lower_lim, upper_lim])\n",
    "    table = tabulate(data, headers=['Outliers (Previously)', 'Outliers', 'Count', 'Column', 'Lower Limit', 'Upper Limit'], tablefmt='rst', numalign='right')\n",
    "    print(\"Removing Outliers using IQR\")\n",
    "    print(table)\n",
    "    \n",
    "replace_with_thresh(df, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a2333-e591-41f2-b02e-316ec115829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "def box_plots(num_cols, df):\n",
    "    for i in range(len(num_cols)-1):\n",
    "        plt.figure(figsize=(10, 5), dpi = 100)\n",
    "        sns.boxplot(x = df[num_cols[i]])\n",
    "\n",
    "# Distribution plots\n",
    "def dis_plots(num_cols, df): \n",
    "    for i in range(len(num_cols)-1):\n",
    "        plt.figure(figsize=(10, 5), dpi = 100)\n",
    "        sns.displot(x = df[num_cols[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc37b3-8e28-4364-845d-7d882a975668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the boxplots of all the columns\n",
    "box_plots(numeric_columns, df)\n",
    "\n",
    "dis_plots(num_cols, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f814e6-6e2a-4791-b84b-e5f01774e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing values of numeric columns with median\n",
    "df_num.apply(lambda x: x.fillna(x.median()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d79e8-88df-47cc-a4f0-c1b6bab5f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers of specific columns !!! ONLY IF VALUES ARE TO BE REPLACED BY MEAN\n",
    "def remove_outliers(df_in, col):    \n",
    "    for x in [col]:\n",
    "        q75,q25 = np.percentile(df_in.loc[:,x],[75,25])\n",
    "        intr_qr = q75-q25\n",
    "\n",
    "        max = q75+(1.5*intr_qr)\n",
    "        min = q25-(1.5*intr_qr)\n",
    "\n",
    "        df_in.loc[df_in[x] < min,x] = np.nan\n",
    "        df_in.loc[df_in[x] > max,x] = np.nan\n",
    "        \n",
    "        mean_val = df[col].mean()\n",
    "        df_in[col].fillna(value=mean_val, inplace=True)\n",
    "        \n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02eff7-17c7-4630-9bf0-aa74df2ad092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from 1 column\n",
    "new_df = remove_outliers(df, 'Column1')\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4fed2e-d6fd-4dc8-8f2d-b91b6368fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from entire dataset !!! ONLY IF VALUES ARE TO BE REPLACED BY MEAN\n",
    "z_scores = stats.zscore(df)\n",
    "\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "new_df = df[filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301029d8-b40d-4621-a3c5-91732923f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing values of numeric columns with mean\n",
    "df_num.apply(lambda x: x.fillna(x.mean()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0458d6-82e9-489f-90ae-8ad0d97c510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing columns using standard interpolation methods\n",
    "df.interpolate(method ='linear', limit_direction ='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18539bc0-0153-49e2-ac0d-6f4b08f52ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing columns using standard interpolation methods\n",
    "df.interpolate(method ='linear', limit_direction ='backward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3878548-389d-4024-8c30-84d600a3121e",
   "metadata": {},
   "source": [
    "Imputing Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a7e1c-5965-4f9f-bb99-0505b4617665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use groupby and apply the value_counts function to extract the most common c15 value within each group:\n",
    "df.groupby('year').apply(lambda x: x['c15'].value_counts().idxmax())\n",
    "# OR\n",
    "df.groupby('Column1')['Column2'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472dc29-8df3-4e42-97ad-fe9cc91d315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing data using KNN\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=n)\n",
    "imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb33fb4-9af3-4b88-b578-ff99551d285f",
   "metadata": {},
   "source": [
    "Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e55abc-58a4-4bc6-a0e7-ed3edc02d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting heatmap\n",
    "import seaborn as sns\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap=\"Blues\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b0f7b-c122-41d0-b5ea-b00e36deab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the heatmap we can construct scatter plots and hist plots \n",
    "# to get a better understanding of the relation between columns\n",
    "import seaborn as sns\n",
    "# Create the default pairplot\n",
    "sns.pairplot(df, hue='species', size=2.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c86da-46fe-48b8-b915-cd2131e84055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want ot look at columns individually\n",
    "df.plot.scatter(x='Col1', y='Col2',  figsize=(15,10), sharex=False,\n",
    "                     title='Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69b97b-5364-4089-85d1-08f1f3b198d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Histogram subplots for each column\n",
    "\n",
    "# get columns to plot\n",
    "columns = df.columns\n",
    "# create x data\n",
    "x_data = range(0, df.shape[0])\n",
    "# create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "# plot each column\n",
    "for column in columns:\n",
    "    ax.plot(x_data, df[column], label=column)\n",
    "    # set title and legend\n",
    "    ax.set_title('Iris Dataset')\n",
    "    ax.legend()\n",
    "\n",
    "# OR\n",
    "\n",
    "df.plot.hist(subplots=True, layout=(2,2), figsize=(10, 10), bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60671329-c7e1-4b53-bbcd-efef24b6b7b7",
   "metadata": {},
   "source": [
    "Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee425f6c-a460-437c-8478-616d86682af1",
   "metadata": {},
   "source": [
    "        T-Test\n",
    "        Between 2 Numerical columns, to determine whether two variables are dependent on each \n",
    "        other or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb54e1-4920-450f-8f36-7bc6ba0e6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test\n",
    "\n",
    "def get_num_cols(df):\n",
    "    types_map = df.dtypes.to_dict()\n",
    "    num_columns = []\n",
    "    for k,v in types_map.items():\n",
    "        if np.issubdtype(np.int64, v) or np.issubdtype(np.float64, v):\n",
    "            num_columns.append(k)\n",
    "\n",
    "    print(num_columns)\n",
    "    return num_columns\n",
    "\n",
    "def t_test(df):\n",
    "    num_columns = get_num_cols(df)\n",
    "    for i in range(len(num_columns)-1):\n",
    "        print(\"\\n --------\"+num_columns[i]+\"--------\")\n",
    "        for j in range(i+1,len(num_columns)):\n",
    "            col1 = num_columns[i]\n",
    "            col2 = num_columns[j]\n",
    "            t_val, p_val = stats.ttest_ind(df[col1], df[col2])\n",
    "            print(\"(%s,%s) => t-value=%s, p-value=%s\" % (num_columns[i], num_columns[j], str(t_val), str(p_val)))\n",
    "            # interpret p-value\n",
    "            alpha = 0.05\n",
    "            #print('significance=%.3f, p=%.3f' % (alpha, p_val))\n",
    "            if p_val <= alpha:\n",
    "                print('Dependent (reject H0)')\n",
    "            else:\n",
    "                print('Independent (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f476484-2d01-47ea-815e-f96c19cb6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8e071-ac85-4580-ba4a-4e0d3bc2965f",
   "metadata": {},
   "source": [
    "        Chi-Square Test \n",
    "        Between 2 categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b7056-bd6c-40e5-90b1-a7d08bd3f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def chi_square_test(cat_cols, df):\n",
    "    for i in range(len(cat_cols)-1):\n",
    "        print(\"\\n\\t --------\"+cat_cols[i]+\"--------\")\n",
    "        for j in range(i+1,len(cat_cols)):\n",
    "            cat_var1 = cat_cols[i]\n",
    "            cat_var2 = cat_cols[j]\n",
    "            data_crosstab = pd.crosstab(df[cat_var1], df[cat_var2], margins = False) \n",
    "            print(data_crosstab) \n",
    "\n",
    "            stat, p, dof, expected = chi2_contingency(data_crosstab)\n",
    "            print('dof=%d' % dof)\n",
    "            print(expected)\n",
    "\n",
    "    # interpret p-value\n",
    "            alpha = 0.05\n",
    "            print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "            if p <= alpha:\n",
    "                print('Dependent (reject H0)')\n",
    "            else:\n",
    "                print('Independent (fail to reject H0)')\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33af0a-f015-4a15-9984-ddb9a196974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_test(cat_cols, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bda87-8e4f-4dfe-b055-36d181fd74a2",
   "metadata": {},
   "source": [
    "        ANOVA\n",
    "        Between 1 numerical and 1 categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a50374-173a-48ea-97ea-48a9dab33d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def anova_test(num_cols, cat_cols, df):\n",
    "    for i in range(len(num_cols)-1):   \n",
    "        num_col = num_cols[i]\n",
    "        for j in range(len(cat_cols)-1):\n",
    "            cat_col = cat_cols[j]\n",
    "            mod = ols(num_col+\" ~ \"+cat_col,data=df).fit()\n",
    "            aov_table=sm.stats.anova_lm(mod, typ=2)\n",
    "            print (\"\\nANOVA => \"+num_col+\" - \"+cat_col)\n",
    "            print(aov_table)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696c6b2-13dd-442f-8582-51e5414251a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_test(num_cols, cat_cols, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185c5a1-58f0-4024-9545-336b132c6340",
   "metadata": {},
   "source": [
    "        Tukey-Test\n",
    "        Between 1 numerical and other categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb7ec0-7e1a-459c-843c-201a7195c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "\n",
    "def tukeyhsd_test(num_cols, cat_cols, df):\n",
    "    for i in range(len(num_cols)-1):\n",
    "        num_var = num_cols[i]\n",
    "        print(\"\\t-------------\"+num_var+\"-------------\")\n",
    "        for j in range(len(cat_cols)-1):\n",
    "            cat_var = cat_cols[j]\n",
    "            tukey = pairwise_tukeyhsd(endog=df[num_var], groups=df[cat_var], alpha=0.05)\n",
    "            #display results\n",
    "            \n",
    "            print(cat_var)\n",
    "            print(tukey)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2e254-7c0e-464e-b13f-ea32a1233058",
   "metadata": {},
   "outputs": [],
   "source": [
    "tukeyhsd_test(num_cols, cat_cols, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b7161-7f15-4f6d-8898-4b8bba074aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling the dataset using Pandas' Profiling Library\n",
    "from pandas_profiling import ProfileReport\n",
    "prof = ProfileReport(df)\n",
    "prof.to_file(output_file='output.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
